{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb5903c",
   "metadata": {},
   "source": [
    "# Programming for Chemistry 2025/2026 @ UniMI\n",
    "\n",
    "![logo](logo_small.png \"Logo\")\n",
    "\n",
    "## Lecture 14: SciPy\n",
    "\n",
    "**SciPy** is a collection of mathematical algorithms and convenience functions built on *NumPy*. SciPy is organized into subpackages covering different scientific computing domains. These are summarized in the following table:\n",
    "\n",
    "|Subpackage            |Description and User Guide|\n",
    "|---|---|\n",
    "|``cluster``           |Clustering algorithms|\n",
    "|``constants``         |Physical and mathematical constants|\n",
    "|``differentiate``     |Finite difference differentiation tools|\n",
    "|``fft``               |Fast Fourier Transforms (FFT)|\n",
    "|``integrate``         |Multidimensional and ODE integration |\n",
    "|``interpolate``       |Multidimensional interpolation|\n",
    "|``io``                |Input/ouput special file formats|\n",
    "|``linalg``            |Extra linear algebra routines|\n",
    "|``ndimage``           |(Multidimensional) image processing|\n",
    "|``odr``               |Orthogonal distance regression|\n",
    "|``optimize``          |Optimization, minimization|\n",
    "|``signal``            |Signal processing|\n",
    "|``sparse``            |Sparse arrays and sparse linear algebra|\n",
    "|``spatial``           |Spatial and geometric algorithms|\n",
    "|``special``           |Special functions|\n",
    "|``stats``             |Statistics|\n",
    "\n",
    "In this lecture I will not cover all of SciPy, but I will just illustrate three sub-packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835615d",
   "metadata": {},
   "source": [
    "## 1. Getting started with SciPy\n",
    "Scipy can be imported by `import scipy`. Each subpackage can be imported with `import scipy.cluster` as an example. You can also import individual functions such as `from scipy.special import sph_harm`. \n",
    "\n",
    "Since SciPy depends heavily on NumPy, we need also to ``import numpy as np``. We also import Matplotlib to visualize the results.\n",
    "\n",
    "SciPy should be already installed in Anaconda. Under Linux/WSL you can install the official packages from your distribution. Otherwise, you can install any version of NumPy in a virtual environment using `pip` or `conda`.\n",
    "\n",
    "Typically one does:\n",
    "```bash\n",
    "conda create myenvinronment\n",
    "conda activate myenvironment\n",
    "conda install scipy\n",
    "```\n",
    "or\n",
    "```bash\n",
    "python -m venv myenvironment\n",
    ". myenvironment/bin/activate\n",
    "pip install scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaad9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccaea9e",
   "metadata": {},
   "source": [
    "## 2. Optimization and curve fitting\n",
    "Computational chemistry/physics is mostly about **optimizing** energies, structures and physical properties molecules or materials.\n",
    "\n",
    "The `scipy.optimize` subpackage provides functions perform bound/unbound optimization of multidimensional objective functions. If the objective function is a sum of squares of the residuals $\\chi^2 = \\sum_i||f(x_i) - y_i||^2$ this is equivalent to **least squares fitting**. We have already seen polynomial fitting in the previous lecture on. With `scipy.optimize` we can fit general functions to data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bdd889",
   "metadata": {},
   "source": [
    "### 2.1 Optimization\n",
    "Let's find the minimum of a complicate function (the Müller-Brown function) which is an example of a multidimensional Potential Energy Surface (PES) with multiple minima and saddle points.\n",
    "\n",
    "The function is: $f(x,y) = \\sum_{i=1}^4 A_i \\exp(a_i(x-x_i)^2 + b_i(x-x_i)(y-y_i) + c_i(y-y_i)^2)$\n",
    "\n",
    "Let's do some plots before minimizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Müller-Brown PES\n",
    "def muller_brown(x, y):\n",
    "    A = [-200, -100, -170, 15]\n",
    "    a = [-1, -1, -6.5, 0.7]\n",
    "    b = [0, 0, 11, 0.6]\n",
    "    c = [-10, -10, -6.5, 0.7]\n",
    "    xi = [1, 0, -0.5, -1]\n",
    "    yi = [0, 0.5, 1.5, 1]\n",
    "\n",
    "    pes = 0.0\n",
    "    for i in range(len(A)):\n",
    "        xx = x - xi[i]\n",
    "        yy = y - yi[i]\n",
    "        pes += A[i] * np.exp(a[i]*xx*xx + b[i]*xx*yy + c[i]*yy*yy)\n",
    "\n",
    "    return pes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a contour plot\n",
    "fig = plt.figure(figsize=(4.5,4))\n",
    "\n",
    "x = np.linspace(-2, 1.5, 100)\n",
    "y = np.linspace(-1.5, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pes = muller_brown(X, Y)\n",
    "\n",
    "plt.contourf(X, Y, pes, levels=np.arange(-150,300,15))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cb52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting point\n",
    "x0, y0 = 0.6, 1.4\n",
    "\n",
    "# minimize (BFGS is the default method)\n",
    "res = minimize(lambda p: muller_brown(p[0],p[1]), x0=(x0,y0))\n",
    "print(res)\n",
    "\n",
    "# final point\n",
    "xf, yf = res.x\n",
    "\n",
    "# plot them:\n",
    "fig = plt.figure(figsize=(4.5,4))\n",
    "plt.contourf(X, Y, pes, levels=np.arange(-150,300,15))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.scatter(x0, y0, color='red', label='start')\n",
    "plt.scatter(xf, yf, color='green', label='final')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0695c7d",
   "metadata": {},
   "source": [
    "### Exercise: copy the cell above and modify it to ...\n",
    "...run the minimization 100 times from a random starting point (use `random.uniform(min,max)`). Try differente methods: `BFGS`, `CG`, `Powell`, `Nelder-Mead`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd95c1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "method = 'Nelder-Mead'\n",
    "\n",
    "# lists of starting points and final points\n",
    "xs, ys = [], []\n",
    "xf, yf = [], []\n",
    "for i in range(100):\n",
    "    ...\n",
    "   \n",
    "# plot them\n",
    "fig = plt.figure(figsize=(4.5,4))\n",
    "plt.contourf(X, Y, pes, levels=np.arange(-150,300,15))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.scatter(...)\n",
    "plt.scatter(...)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b3faf",
   "metadata": {},
   "source": [
    "### 2.2 Fitting\n",
    "SciPy provides a powerful `scipy.optimize.curve_fit()` function which can use different algorithm to perform both constrained and unconstrained fitting.\n",
    "\n",
    "Let's prepare some data points to be fitted. We generate them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e990426",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "xdata = np.arange(0, 10)\n",
    "ydata = 5*np.random.random(10) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ebd23",
   "metadata": {},
   "source": [
    "In order to fit, we have to create a function taking `x` and all fitting parameters as arguments. Remember to use math functions from NumPy, not from `math`. We can also use an anonymous `lambda` function.\n",
    "\n",
    "Let's fit and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b, c):\n",
    "    return a * np.exp(b*x) + c\n",
    "\n",
    "popt, pcov = curve_fit(f, xdata, ydata)\n",
    "print('optimized parameters (a,b,c):', popt)\n",
    "print('sigma of opt. params (a,b,c):', np.sqrt(np.diag(pcov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ce6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "x = np.linspace(0, 10, 100)\n",
    "plt.scatter(xdata, ydata, label='data')\n",
    "plt.plot(x, f(x, *popt), label='fit')      # *popt \"unpacks\" the array into a, b, c\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e50db17",
   "metadata": {},
   "source": [
    "Let's try other functions. In particular let's verify the urban legend that *\"with three gaussians you can fit every data set\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b, c, d):\n",
    "    return a*np.cos(b*x) + c*x + d\n",
    "\n",
    "popt, pcov = curve_fit(f, xdata, ydata)\n",
    "print('optimized parameters:', popt)\n",
    "print('sigma of opt. params:', np.sqrt(np.diag(pcov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1697de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "x = np.linspace(0, 10, 100)\n",
    "plt.scatter(xdata, ydata, label='data')\n",
    "plt.plot(x, f(x, *popt), label='fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x, a, mu, sigma):\n",
    "    return a*np.exp(-(x-mu)**2/sigma)\n",
    "\n",
    "def f(x, a1,b1,c1, a2,b2,c2, a3,b3,c3):\n",
    "    return gauss(x,a1,b1,c1) + gauss(x,a2,b2,c3) + gauss(x,a3,b3,c3)\n",
    "\n",
    "popt, pcov = curve_fit(f, xdata, ydata, p0=(1,2,0.5, -1,4,0.5, 1,8,0.5), method='trf')\n",
    "print('optimized parameters:', popt)\n",
    "print('sigma of opt. params:', np.sqrt(np.diag(pcov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141de9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "x = np.linspace(0, 10, 100)\n",
    "plt.scatter(xdata, ydata, label='data')\n",
    "plt.plot(x, f(x, *popt), label='fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54c68d",
   "metadata": {},
   "source": [
    "This is not working well, but maybe, with four gaussians..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a862111",
   "metadata": {},
   "source": [
    "## 3. Interpolation\n",
    "Interpolation is different from fitting. In fitting the curve minimizes the quadratic difference with the data points. In interpolation instaed, the curve passes exactly through the points.\n",
    "\n",
    "Let's try to interpolate the same set of data using piece-wise polynomial functions. The `scipy.interpolate.interp1d(xdata,ydata,...)` **returns a function** that interpolated the data points.\n",
    "\n",
    "`scipy.interpolate` can also interpolate the data using **cubic splines**, **B-splines**, and many other, also in multiple dimensions. In particular cubic splines are continous up to the second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b89597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import CubicSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "x = np.linspace(0, 9, 100)\n",
    "plt.scatter(xdata, ydata, label='data')\n",
    "\n",
    "f = interp1d(xdata, ydata, kind='linear')\n",
    "plt.plot(x, f(x), label='linear interp.')\n",
    "\n",
    "f = interp1d(xdata, ydata, kind='quadratic')\n",
    "plt.plot(x, f(x), label='quad. interp.')\n",
    "\n",
    "f = CubicSpline(xdata, ydata)\n",
    "plt.plot(x, f(x), label='cubic spline')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883d1ed",
   "metadata": {},
   "source": [
    "## 4. Find peaks in data\n",
    "The `scipy.signal` sub-package provides poweful functions for **signal processing**. In the section we will analyze Powder X-ray Diffraction Data (PXRD) of micro- and nano-structured CeO$_2$ kindly provided by Prof. M. Scavini.\n",
    "\n",
    "The PXRD data has been recorded at the ESRF synchrotron with a wavelength of 0.3543800 Å. The `.xye` files contain three columns: $2\\theta$, intensity, intensity uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748f67a",
   "metadata": {},
   "source": [
    "### 4.1 Load the data sets...\n",
    "...and plot the diffractograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578381b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ceo2_micro = np.loadtxt('ch5157_CeO2_900C_090K_10mm_003.xye', usecols=(0,1))\n",
    "ceo2_nano1 = np.loadtxt('ch5157_CeO2E_500C_090K_10mm_005.xye', usecols=(0,1))\n",
    "ceo2_nano2 = np.loadtxt('ch5157_CeO2E_200C_090K_10mm_010.xye', usecols=(0,1))\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ceo2_micro[:,0], ceo2_micro[:,1], label='micro')\n",
    "plt.xlabel(\"$2\\\\theta$ (°)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ceo2_nano1[:,0], ceo2_nano1[:,1], label='nano1')\n",
    "plt.plot(ceo2_nano2[:,0], ceo2_nano2[:,1], label='nano2')\n",
    "plt.xlabel(\"$2\\\\theta$ (°)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e3888",
   "metadata": {},
   "source": [
    "Let's use `scipy.signal.find_peaks` on the microstructured ceria PXRD. `find_peaks` returns a list of array indexes corresponding to the peaks.\n",
    "\n",
    "Let's plot them and let's tune the parameters of `find_peaks` to find only the relevant peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "twotheta = ceo2_micro[:,0]\n",
    "pxrd = ceo2_micro[:,1]\n",
    "\n",
    "#peaks, props = find_peaks(pxrd, height)\n",
    "peaks, props = find_peaks(pxrd, height=5000)\n",
    "print('peaks indexes:', peaks)\n",
    "\n",
    "# extract the 2*theta of the peaks\n",
    "twotheta_peaks = twotheta[peaks]\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.plot(twotheta, pxrd, label='micro')\n",
    "plt.scatter(twotheta_peaks, pxrd[peaks], marker='X', color='orange')\n",
    "plt.xlabel(\"$2\\\\theta$ (°)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c6c0d",
   "metadata": {},
   "source": [
    "The next step is to transform the data from $2\\theta$ in $|Q|$ according to:\n",
    "\n",
    "$|Q| = \\frac{4\\pi}{\\lambda} sin\\theta$\n",
    "\n",
    "Write a function that takes a $2\\theta$ *ndarray* and the wavelength and returns $|Q|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = 0.35438    # angstrom\n",
    "\n",
    "def twotheta_to_q(twotheta, wave):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb247f",
   "metadata": {},
   "source": [
    "CeO$_2$ is cubic with space group Fm-3m. In the *fcc* crystal there are systematic extinctions. The allowed Miller indexes $(h,k,l)$ are either all even or all odd.\n",
    "Let's index the peaks:\n",
    "\n",
    "1. create a list of (h,k,l) tuples, with h >= k >= l, all even or all odd values up to h,k,l <= 5, h=k=l=0 excluded\n",
    "2. sort the list in order of increasing modulus $h^2+k^2+l^2$ using the `key` option of the `sort` method\n",
    "3. calculate the lattice parameter from the first 10 peaks according to the formula:\n",
    "\n",
    "$a_i = \\frac{2\\pi}{Q_i} \\sqrt{h^2+k^2+l^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ec94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hkl = []\n",
    "\n",
    "...\n",
    "\n",
    "print(\"before sorting:\")\n",
    "print(hkl)\n",
    "\n",
    "hkl.sort(key=...)\n",
    "print(\"after sorting:\")\n",
    "print(hkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from 2*theta to q\n",
    "q = twotheta_to_q(twotheta_peaks, wavelength)\n",
    "\n",
    "hkl = np.array(hkl)\n",
    "print(hkl.shape)\n",
    "\n",
    "reflections = 10\n",
    "a = 2*np.pi / q[0:reflections] * np.linalg.norm(hkl[0:reflections,:],axis=1)\n",
    "\n",
    "for i in range(reflections):\n",
    "    print(f'peak #{i:2}  q={q[i]:10.4f}  a={a[i]:10.6f}')\n",
    "\n",
    "print()\n",
    "print(f'average lattice parameter: {np.mean(a):.4f} ± {np.std(a):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c31f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
