{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb5903c",
   "metadata": {},
   "source": [
    "# Programming for Chemistry 2025/2026 @ UniMI\n",
    "\n",
    "![logo](logo_small.png \"Logo\")\n",
    "\n",
    "## Lecture 16: Machine Learning with Scikit-Learn\n",
    "\n",
    "Machine learning (ML) is a hot topic with popular applications in driverless cars, internet search engines, and data analysis among many others. Numerous fields are utilizing machine learning, and chemistry is certainly no exception, with papers using machine learning methods being published regularly. There is a considerable amount of hype around the topic along with debate about whether the field will live up to this hype. However, there is little doubt that machine learning is making a significant impact and is a powerful tool when used properly.\n",
    "\n",
    "**Machine learning** occurs when a program exhibits behavior that is not explicitly programmed but rather is **learned** from data.\n",
    "\n",
    "Most of the material of this lecture is taken from this [lecture notes](https://weisscharlesj.github.io/SciCompforChemists/notebooks/chapter_13/chap_13_notebook.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e264bb6",
   "metadata": {},
   "source": [
    "**Scikit-Learn** is a collection of algorithms built on top of *NumPy*, *SciPy* and *Pandas*. **Pandas** is a package for data analysis and manipulation. In this course I will not enter into the details of the Pandas package, since we will just use it's basic functions to load data from various file formats.\n",
    "\n",
    "Scikit-Learn (aka **sklearn**) is organized into subpackages covering different machine learning techniques. The most important are:\n",
    "\n",
    "* Preprocessing\n",
    "* Regression\n",
    "* Classification\n",
    "* Dimensionality reduction\n",
    "* Clustering\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd3d27",
   "metadata": {},
   "source": [
    "## 1. Getting started with sklearn\n",
    "Sklearn can be imported by `import sklearn`. You can also import individual functions such as `from sklearn.linear_regression import LinearRegresion`. \n",
    "\n",
    "Since sklearn depends heavily on NumPy, we need also to ``import numpy as np``. We will also import the Pandas package with `import pandas as pd`. We also import Matplotlib to visualize the results.\n",
    "\n",
    "Scikit-learn should be already installed in Anaconda. Under Linux/WSL you can install the official packages from your distribution. Otherwise, you can install any version of NumPy in a virtual environment using `pip` or `conda`.\n",
    "\n",
    "Typically one does:\n",
    "```bash\n",
    "conda create myenvinronment\n",
    "conda activate myenvironment\n",
    "conda install scikit-learn pandas\n",
    "```\n",
    "or\n",
    "```bash\n",
    "python -m venv myenvironment\n",
    ". myenvironment/bin/activate\n",
    "pip install scikit-learn pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cc035",
   "metadata": {},
   "source": [
    "## 2. Supervised learning\n",
    "*Supervised learning* is where the machine learning algorithms are provided with both **feature** and **target** information with the goal of developing a model to predict targets based on the features. When the supervised machine learning predictions are looking to categorize an item like a photo or type of metal complex, it is known as **classification**; and when the predictions are seeking a numerical value from a continuous range, it is a **regression** problem. Some machine learning algorithms are designed for only classification or only regression while others can do either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6860022",
   "metadata": {},
   "source": [
    "### 2.1 Features and Information\n",
    "\n",
    "The file titled `ROH_data.csv` contains information on over seventy simple alcohols (i.e., a single -OH with no other non-hydrocarbon function groups) including their boiling points. Our goal is to generate a function or algorithm to predict the boiling points of the alcohols based on the information on the alcohols, so here the target is the boiling point and features are the other information about the alcohols.\n",
    "\n",
    "The dataset includes the boiling point (K), molecular weight (g/mol), number of carbon atoms, whether or not it is aliphatic, degree, whether it is cyclic, and the average position of any aryl substituents. Scikit-learn requires that all features be represented numerically, so for the last three features `1` represents `True` and `0` represents `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c364220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a data set of alchools\n",
    "ROH = pd.read_csv('data/ROH_data.csv', sep=',')\n",
    "\n",
    "# display the beginning of the data set\n",
    "ROH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a23bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a summary of the data set\n",
    "ROH.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122e3a7",
   "metadata": {},
   "source": [
    "Let's plot the boiling point with respect to the other quantities in the data set, in order to see is the is any correlation. Note that Pandas is fully compatible with Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(ROH['MW'], ROH['bp'])\n",
    "plt.xlabel('Mol. weight (g/mol)')\n",
    "plt.ylabel('Boiling point (K)')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.scatter(ROH['MW'], ROH['carbons'])\n",
    "plt.xlabel('Number of carbons')\n",
    "plt.ylabel('Boiling point (K)')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.scatter(ROH['MW'], ROH['avg_aryl_position'])\n",
    "plt.xlabel('Avg. aryl pos.')\n",
    "plt.ylabel('Boiling point (K)')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.scatter(ROH['MW'], ROH['cyclic'])\n",
    "plt.xlabel('is cyclic')\n",
    "plt.ylabel('Boiling point (K)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7faa5b",
   "metadata": {},
   "source": [
    "### 2.2 Train-test split\n",
    "Whenever training a machine learning model to make predictions, it is important to evaluate the accuracy of the predictions. It is unfair to test an algorithm on data it has already seen, so before training a model, first **split** the dataset into a training subset and a testing subset. It is also important to shuffle the dataset before splitting it as many datasets are at least partially ordered.\n",
    "\n",
    "Scikit-learn provides a built-in function for shuffling and splitting the dataset known as `train_test_split()`. The arguments are the features, target, and the fraction of the dataset to be used for testing. Below, a quarter of the dataset is allotted for testing (`test_size=0.25`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target is the property we are going to model (fit)\n",
    "target = ROH['bp']\n",
    "\n",
    "# features are the variables targets depends on\n",
    "features = ROH[ ['MW', 'carbons', 'degree', 'aliphatic', 'avg_aryl_position','cyclic'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state is like a random seed, to make results reproducible\n",
    "# by convention we use capital X or Y if they are multidimensional,\n",
    "# lower letter x or y if they are one-dimensional data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=18)\n",
    "\n",
    "print(\"training data points:\", len(X_train))\n",
    "print(\"testing data points :\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42159b9",
   "metadata": {},
   "source": [
    "### 2.3 Linear regression\n",
    "\n",
    "Now for some machine learning using a very simple *linear regression* model. This model treats the target value as a linear combination or weighted sum of the features where $x$ are the features and $w$ are the weights.\n",
    "\n",
    "$$ target = w_0X_0 + w_1X_1 + w_2X_2 + w_3X_3 + w_4X_4 + w_5X_5 + ... $$\n",
    "\n",
    "The general procedure for supervised machine learning, regardless of model, usually includes three steps.\n",
    "\n",
    "1. Create a model and attach it to an *object*\n",
    "2. Train the model with the training data\n",
    "3. Evaluate the model using the testing data or use it to make predictions\n",
    "\n",
    "To implement these steps, the linear model from the `linear_model` module is first created with the `LinearRegression()` function and assigned the variable `reg`. Next, it is trained using the `fit()` method and the training data from above.\n",
    "\n",
    "Finally, it is important to evaluate the effectiveness of trained machine learning models before rolling them out for widespread use, and scikit-learn provides multiple built-in functions to help in this task. The first is the `score()` method. Instead of making predictions using the testing features and then plotting the predictions against the known values, the `score()` method takes in the testing features and target values and returns the $r^2$. The closer the $r^2$ value is to 1, the better the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52331b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35135e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print the first four predicted values\n",
    "prediction = reg.predict(X_test)\n",
    "print(\"predicted boiling points:\", prediction[0:4])\n",
    "print(\"true boiling points     :\", np.array(y_test[0:4]))\n",
    "\n",
    "# the model score is:\n",
    "print('model score=', reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the predicted boiling points\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.scatter(prediction, y_test)\n",
    "plt.plot(y_test, y_test, linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Predicted Boiling Point (K)')\n",
    "plt.ylabel('True Boiling point (K)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b3e6e",
   "metadata": {},
   "source": [
    "### 2.4 Coefficients\n",
    "Recall that the linear model calculates the boiling point based on a weighted sum of the features, so it can be informative to know the weights to see which features are the most influential in making the predictions. The `LinearRegression()` method contains the attribute `coef_` which provides these coefficients in a NumPy array.\n",
    "\n",
    "These coefficients correspond to molecular weight, number of carbons, degree, whether or not it is aliphatic, average aryl position, and whether or not it is cyclic, respectively. While some coefficients are larger than others, we cannot yet distinguish which features are more important than the others because the values for each feature occur in different ranges. This is because the coefficients are not only proportional to the predictive value of a feature but also inversely proportional to the magnitude of feature values. For example, while the molecular mass has greater predictive value than the degree, the degree has a larger coefficient because it occurs in a smaller range (1 $\\rightarrow$ 3) than the molecular weights (32.04 $\\rightarrow$ 186.33 g/mol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that the feature of the model are:\n",
    "print(features.head())\n",
    "\n",
    "print(\"coefficents=\", reg.coef_)\n",
    "print(\"intercept  =\", reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed2050",
   "metadata": {},
   "source": [
    "### 2.5 Preprocessing, scaling\n",
    "To understanf which coefficient is more important for the fit, we need to scale the features to the same range. Three common feature scaliess this issue, the scikit-learn `sklearn.preprocess` module provides a selection of functions to scale or normalize the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbf3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features)\n",
    "scaled_features = scaler.transform(features)\n",
    "\n",
    "print(scaled_features[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's repeat the linear regression and check the coefficients\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.25, random_state=18)\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3da93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is clear that the first two coefficients are the most important\n",
    "print(\"coefficents=\", reg.coef_)\n",
    "print(\"intercept  =\", reg.intercept_)\n",
    "print('model score=', reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e8a3e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let's play with the `features`. Try to remove some of them, perform the linear regression and see if the model score improves or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target is the property we are going to model (fit)\n",
    "target = ROH['bp']\n",
    "\n",
    "# features are the variables targets depends on\n",
    "features_list = ['MW', 'carbons', 'degree', 'aliphatic', 'avg_aryl_position', 'cyclic']\n",
    "features_list.remove('cyclic')\n",
    "features_list.remove('degree')\n",
    "features_list.remove('carbons')\n",
    "features_list.remove('MW')\n",
    "\n",
    "print('feature_list=', features_list)\n",
    "features = ROH[ features_list ]\n",
    "\n",
    "# scale the features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features)\n",
    "scaled_features = scaler.transform(features)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.25, random_state=18)\n",
    "\n",
    "# perform the linear regression\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"coefficents=\", reg.coef_)\n",
    "print(\"intercept  =\", reg.intercept_)\n",
    "print('model score=', reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d375db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted boiling points\n",
    "prediction = reg.predict(X_test)\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.scatter(prediction, y_test)\n",
    "plt.plot(y_test, y_test, '-', lw=1.3, alpha=0.5)\n",
    "\n",
    "plt.xlabel('Predicted Boiling Point (K)')\n",
    "plt.ylabel('True Boiling point (K)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f37fa",
   "metadata": {},
   "source": [
    "### 2.6 Classification of discrete variables: random forests\n",
    "If the target property is a **discrete** variable instead of a continous variable (i.e. the boiling point), we will perfom **classifiaction**. In this examles classification involves sorting items into discrete categories such as sorting **alcohols**, **aldehydes/ketones**, and **amines** by type based on features. \n",
    "\n",
    "Scikit-learn provides a number of algorithms for classification. One method is the **decision tree** which sorts items into categories based on a series of  conditions. You can think it as a series of `if/elif/else` conditions based on the dataset features. For instance:\n",
    "\n",
    "* `if` degrees of unsaturation `> 0` most likely aldehydes or ketones `else` alcohols or amines\n",
    "* ...then you should work out yourself other conditions like this\n",
    "\n",
    "**This is the power of machine learning:** decitions trees and random forests (i.e. training a large ensemble of decision trees on different subsets of the dataset) is let the computer learn from data and write code for you.\n",
    "\n",
    "![random_forest](random_forest.png \"Random forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137e02a",
   "metadata": {},
   "source": [
    "### 2.7 Classifiy chemical compounds\n",
    "We will use a small dataset containing 122 monofunctional organic compounds from three different categories: **alcohols (category 0)**, **ketones/aldehydes (category 1)**, and **amines (category 2)**. The **features** provided are the *molecular weight*, *number of carbons*, *boiling point*, whether it is *cyclic*, whether it is *aromatic*, and the *unsaturation number*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400cd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/org_comp.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's choose the target and the features\n",
    "target = data['class']\n",
    "features = data[ ['bp', 'MW', 'C', 'cyclic', 'aromatic', 'unsaturation'] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual we split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's classify using a RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "prediction = rf.predict(X_test)\n",
    "print('model score=', rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3783e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted molecule class\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.scatter(prediction, y_test)\n",
    "\n",
    "plt.xlabel('Predicted class')\n",
    "plt.xticks(ticks=[0,1,2], labels=['alchool', 'keto/ald', 'amine'])\n",
    "plt.yticks(ticks=[0,1,2], labels=['alchool', 'keto/ald', 'amine'])\n",
    "plt.ylabel('True class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf72189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case it is better to plot the confusion matrix,\n",
    "# which tells us how many times the predicted class was correct\n",
    "# or incorrect\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf = confusion_matrix(y_test, prediction)\n",
    "print(conf.shape)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb09bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the confusion matrix\n",
    "plt.figure(figsize=(4.5,4))\n",
    "\n",
    "plt.imshow(conf, cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('Predicted class')\n",
    "plt.xticks(ticks=[0,1,2], labels=['alchool', 'keto/ald', 'amine'])\n",
    "plt.yticks(ticks=[0,1,2], labels=['alchool', 'keto/ald', 'amine'])\n",
    "plt.ylabel('True class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4b525",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let's play with the `features`. Try to remove some of them, perform the linear regression and see if the model score improves or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d857e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's choose the target and the features\n",
    "target = data['class']\n",
    "feature_list = ['bp', 'MW', 'C', 'cyclic', 'aromatic', 'unsaturation']\n",
    "feature_list.remove('bp')\n",
    "feature_list.remove('unsaturation')\n",
    "features = data[ feature_list ] \n",
    "\n",
    "# as usual we split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=18)\n",
    "\n",
    "# let's classify using a RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "prediction = rf.predict(X_test)\n",
    "print('model score=', rf.score(X_test, y_test))\n",
    "\n",
    "conf = confusion_matrix(y_test, prediction)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the confusion matrix\n",
    "plt.figure(figsize=(4.5,4))\n",
    "\n",
    "plt.imshow(conf, cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('Predicted class')\n",
    "plt.xticks(ticks=[0,1,2], labels=['alchool', 'keto/ald', 'amine'])\n",
    "plt.yticks(ticks=[0,1,2], labels=['alchool', 'keto/ald', 'amine'])\n",
    "plt.ylabel('True class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f5fe1",
   "metadata": {},
   "source": [
    "## 3. Unsupervised Learning\n",
    "*Unsupervised learning* is when no target value is provided to the machine learning algorithm. Unsupervised learning seeks to **find patterns** in the data instead of making predictions.\n",
    "\n",
    "One form of unsupervised ML is **dimensionality reduction** where the number of features is condensed down to typically two or three features while maintaining as much information as possible.\n",
    "\n",
    "Another unsupervised learning task is **clustering** where the algorithm attempts to group similar items in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57234fe8",
   "metadata": {},
   "source": [
    "### 3.1 Dimensional Reduction\n",
    "When your dataset has multiple *features*, it is of course difficult to visualize. Let's consider star constellations for a geometrical analogy. Constellations appear to be group of close-by stars on the 2d plane, but actually stars are distant from each other in the 3d space.  \n",
    "\n",
    "![constellation](constellation.png \"Star constellation\")\n",
    "\n",
    "Dimensional reduction aims at finding the **best projection (hyper-)plane** such that items which are close in the high-dimensional space, they are also close in the reduced space.\n",
    "\n",
    "**Principal Component Analysys** forms linear combinations of the features and tries to condense them down to 2-3 *most important* features, that you can visualize.\n",
    "\n",
    "We will use scikit-learnâ€™s `datasets` module, which contains datasets along with data-generating functions. We will use the wine classification dataset that includes 178 samples of three different types of wines, which we will classify based on features such as alcohol content, hue, malic acid, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e5f60",
   "metadata": {},
   "source": [
    "### 3.2 Load and inspect the wine dataset\n",
    "To load the wine dataset, we first need to import the `load_wine()` function and then call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9aec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ceb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wine dataset has several attributes. The data attribute is NumPy array\n",
    "features = wine.data\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns are 13 features, and their names are\n",
    "print(len(wine.feature_names))\n",
    "print(wine.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8dc6bf",
   "metadata": {},
   "source": [
    "### 3.3 Scale the features and peform the PCA\n",
    "It is always best to preprocess the data, since the numeric range of the features may vary by order of magnitude. Here we don't need to split the dataset, because we are not performing any prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# scale the data such that every feature has mean value=0 and standard deviation=1 \n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# reduce to two features\n",
    "pca = PCA(n_components=2)\n",
    "trans_data = pca.fit_transform(scaled_features)\n",
    "print(trans_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the 2-dim features\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(trans_data[:,0], trans_data[:,1])\n",
    "plt.xlabel('PCA component #1')\n",
    "plt.ylabel('PCA component #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the linear combinations of the 13 features\n",
    "components = pca.components_\n",
    "print(components.shape)\n",
    "\n",
    "for j in range(2):\n",
    "    print(f'PCA component #{j+1}: ', end='')\n",
    "    for i in range(len(wine.feature_names)):\n",
    "        feat = wine.feature_names[i]\n",
    "        print(f'{components[j,i]:+.2f}*{feat} ', end='')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892fd03",
   "metadata": {},
   "source": [
    "### 3.4 Clustering\n",
    "Clustering involves grouping similar items in a dataset. The are several clustering algorithms: **k-means**, **agglomerative clustering**, and **Density Based Spatial Clustering Application with Noise (DBSCAN)**. \n",
    "\n",
    "Here we will try to see if the reduced 2-dimensional wine dataset (the `trans_data` NumPy array) can be clustered into **classes** of wine. The clustering algorithms will try to assign a label (from 0 to number of cluster - 1) to each datapoint, that we can use to plot with different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try with two or three clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=18, n_init=\"auto\")\n",
    "kmeans.fit(trans_data)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "print(labels)\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters and their centers\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(trans_data[:,0], trans_data[:,1], c=labels)\n",
    "plt.scatter(centers[:,0], centers[:,1], marker='+', color='black')\n",
    "plt.xlabel('PCA component #1')\n",
    "plt.ylabel('PCA component #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6527797",
   "metadata": {},
   "source": [
    "In reality the wine dataset has a `target` attribute, that stands for the quality of the wine (0=bad, 1=good, 2=excellent). In fact we can use the wine dataset for *supervised learning* as well. Let's compare the *labels* obtained with `KMeans` with the *targets* of the orignal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa980d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = wine.target\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1192558",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(trans_data[:,0], trans_data[:,1], c=target)\n",
    "plt.title('Original wine quality')\n",
    "plt.xlabel('PCA component #1')\n",
    "plt.ylabel('PCA component #2')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(trans_data[:,0], trans_data[:,1], c=labels)\n",
    "plt.title('KMeans wine quality')\n",
    "plt.xlabel('PCA component #1')\n",
    "plt.ylabel('PCA component #2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a199b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally, the confusion matrix\n",
    "conf = confusion_matrix(target, labels)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.5,4))\n",
    "\n",
    "plt.imshow(conf, cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('Original wine quality')\n",
    "plt.xticks(ticks=[0,1,2], labels=['bad', 'good', 'excellent'])\n",
    "plt.ylabel('KMeans wine quality')\n",
    "plt.yticks(ticks=[0,1,2], labels=['class-0', 'class-1', 'class-2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63ca0d",
   "metadata": {},
   "source": [
    "### Final exercise: Random forest on wine data\n",
    "Let's perform a supervised learning using Random forests. Remember to:\n",
    "\n",
    "1. split the dataset into training and testing\n",
    "2. perform the Random Forest and print the score\n",
    "3. calculate the confusion matrix\n",
    "\n",
    "Copy and paste from the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7031e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = wine.target\n",
    "features = wine.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test\n",
    "...\n",
    "\n",
    "# classify using a RandomForest\n",
    "...\n",
    "\n",
    "# calculate the confusion matrix\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(4.5,4))\n",
    "\n",
    "plt.imshow(conf, cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('Predicted wine quality')\n",
    "plt.xticks(ticks=[0,1,2], labels=['bad', 'good', 'excellent'])\n",
    "plt.ylabel('True wine quality')\n",
    "plt.yticks(ticks=[0,1,2], labels=['bad', 'good', 'excellent'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885ebfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
